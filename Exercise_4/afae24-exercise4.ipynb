{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12394aa1-0fe2-49d1-951a-6defb73ef28c",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability, and Ethics, Spring 2024\n",
    "# Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc72700",
   "metadata": {},
   "source": [
    "## Task 0 (Setup)\n",
    "\n",
    "We use the same dataset as in week 2 and 3. If you missed to install the module, please carry out the installation tasks at <https://github.com/zykls/folktables#basic-installation-instructions>.\n",
    "\n",
    "After successful installation, you should be able to run the following code to generate a prediction task.\n",
    "To make your life easier, we made the `BasicProblem`-magic from the `folktables` package (see exercises of week 2) explicit in this task.\n",
    "This way, you can get access to different encodings of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5921f706-cf73-4c13-a7da-0448ef057d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = [ \"HINS1\", \"HINS2\", \"HINS4\", \"CIT\", \"COW\", \"SCHL\", \"MAR\", \"SEX\", \"RAC1P\"]\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=cols, drop_first=True)\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[\"ENG\"], drop_first=True)\n",
    "    return df_processed, df, target, sex\n",
    "\n",
    "data, data_original, target, group = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc7dad",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "1. Train a black-box model classifier (for example, use a random forest, a gradient-boosted decision tree, an SVM, or a Neural Network). Report on its accuracy. If you have used a tree data structure such as RF or gradient-boosted decision trees, report on the feature importance.\n",
    "2. Use the `shap` module to explain predictions on your black-box model. Do the same with a white-box model that you have trained during last week's exercises. Contrast the two models to each other: What are similarities, how do they differ? As shown in the lecture, provide a summary plot, a dependence plot,  a  force plot for a negatively/positively predicted feature vector, and summary plot on the interaction values.\n",
    "3. Reflect on the explanations: How does the white box models's (from last week) black-box explanation relate to its white-box explanation? Which classifier would you prefer when deploying a model as part of the machine learning pipeline? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495613e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "90245f237ffeb45287c33c260238522010c806ee942870b1ad6ce10064a8bcac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
