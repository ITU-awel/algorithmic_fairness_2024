{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*). I have chosen some variables for you, but you can add more (*if you like to*) - here is the [full list](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2021.pdf) of variables (some of them do not exist in `ACSDataSource`). \n",
    "\n",
    "Today we are going to debias a regression model using the `SEX` variable. Your model should predict the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`, you can choose another threshold).\n",
    "\n",
    "\n",
    "* If you code is slow - you can subsample data (aka reduce the number of the samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP', # include AGE\n",
    "        'COW', # include class of worker\n",
    "        'SCHL', # include school education\n",
    "        'WKHP', # include reported working hours\n",
    "        'SEX', # include sex\n",
    "        # some random, possibly noisy\n",
    "        'PWGTP', # person weight\n",
    "        'JWMNP', # travel time to work\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,    \n",
    "    group='SEX',\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small snippet to get the names of the categorical variables - I convert categoricals into one-hot encoded (*you don't have to, depending on what assumptions you use about the data*). **Don't forget to normalise the continious features (if you plan to use Cross-Validation features should be normalized per fold, aka not in the global table).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jv/zphrgwvd6t5c55mwrbmghvj80000gn/T/ipykernel_64600/1004060965.py:7: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  features = features.fillna(-1)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical columns\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "\n",
    "# Fill nas in data with -1s\n",
    "features = features.fillna(-1)\n",
    "\n",
    "# Add intercept column\n",
    "features.insert(0, 'ones', np.ones(len(features)))\n",
    "\n",
    "# Test-train split\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups.values.reshape(-1), test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "# Subsample to make code run faster\n",
    "N = 1000 \n",
    "X_train = X_train[:N]\n",
    "y_train = y_train[:N]\n",
    "\n",
    "# Normalize continious features\n",
    "scaler = RobustScaler(unit_variance=True).fit(X_train[:, 1:5]) ## normalize the first four columns, aka continiou features\n",
    "X_train_scaled = np.concatenate([np.ones((X_train.shape[0],1)), scaler.transform(X_train[:,1:5]), X_train[:,5:]], axis=1)\n",
    "X_test_scaled = np.concatenate([np.ones((X_test.shape[0],1)), scaler.transform(X_test[:,1:5]), X_test[:,5:]], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constrained Regression Model \n",
    "Now let's try to include the [Fairness Constraint](https://arxiv.org/abs/1706.02409)! You'll have to implement couple of things from scratch (as it is tricky to add a custom constraint function in `sklearn`.  To optimise the cost function let's use `scipy.optimize.fmin_tnc`. To calculate gradient you can use `fprime` attribute):\n",
    "1. Logistic Regression\n",
    "2. L2 penalisation\n",
    "3. **Individual** Fairness Constrained\n",
    "\n",
    "When you are finished with implementation - you should evaluate performance on multiple choices of fairness weight, $\\lambda$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed breakdown\n",
    "The INDIVIDUAL constraint constraint looks like this:\n",
    "\n",
    "$$ \n",
    "f(\\beta,S) = \\left( \\frac{1}{n_1 n_2} \\sum_{(x_i,y_i)\\in S_1, (x_j,y_j)\\in S_2} d(y_i,y_j) (\\beta^T  \\textbf{x}_i - \\beta^T \\textbf{x}_j)^2  \\right) \n",
    "$$\n",
    "\n",
    "\n",
    "For the constrained optimization we have to solve a problem on the form:\n",
    "\n",
    "$$ \\min_\\beta \\left( \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\beta \\Vert_2 \\right) $$ \n",
    "\n",
    "where $\\ell$ is some loss function, $f$ is the constraint function, and the $\\gamma \\Vert \\beta \\Vert_2 $ is L2 regularization (we use it to avoid overfitting).\n",
    "(Basically we are minimizing the Lagrangian $\\mathscr{L} = \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\textbf{x} \\Vert_2$ with respect to $\\beta$ - in ML literature $\\mathscr{L}$ is often denoted as J)\n",
    "\n",
    "Because we are doing classification we are going to use logistic regression. The log loss function is:\n",
    "$$\n",
    "\\ell = \\frac{1}{m}\\sum_i^m\\left[ -y_i \\log(g(x_i)) - (1-y_i)\\log(1-g(x_i)) \\right], \\text{where } g(x_i) = \\frac{1}{1+\\exp(-\\beta_i x_i)}\n",
    "$$\n",
    "\n",
    "For the distance function we follow the approach from Berk et al. (2017) and set:\n",
    "$$d(y_i,y_j) = \\begin{cases}\n",
    "            1, &         \\text{if } y_i=y_j,\\\\\n",
    "            0, &         \\text{if } y_i\\neq y_j.\n",
    "    \\end{cases}$$\n",
    "    \n",
    "To minimize the total loss function we also need to estimate the gradient of $\\mathscr{L}$ with respect to $\\beta$. Here to update the $\\beta$ values we are just going the gradient's without the fairness constraing - this will make our lives considerably easier. The j'th element of the gradiend is defined as follows:\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta_j} \\approx \\frac{1}{m}\\left( \\sum_i  (g(x_i) - y_i) x[j] \\right)+ 2\\gamma \\beta_j\n",
    "$$\n",
    "\n",
    "##### A little clarification and tips:\n",
    "1. In order to simplify the exercise - we cut some corners. *Ideally* we should calculate the gradient in respect to the *individual fairness*. The gradient takes into the account only logistic and l2 loss (aka, parameters are updated based on those). At the same time, our *cost* has a *individual fairness* included. When the update of the parameters stops decreasing the cost, the `fmin_tnc` is going to stop optimisation. So our implementation is not entirely correct.\n",
    "2. In case you want to have a more correct implementation, you can do `opt.fmin_tnc(func=compute_cost, x0=betas, fprime = None, approx_grad= True, ...)`. It is quite long, but you can still do it\n",
    "3. I also suggest setting `ftol=1e-5`. \n",
    "4. Don not apply l2-regularization on the intercept (when you calculate the gradient).\n",
    "5. You should include $x_0 = 1$ in your data, for each observation (when it comes to the manual implementation of logistic regression) to include bias (i.e. weight $\\beta_0$).\n",
    "6. To keep the exercise simpler, let's fix $\\gamma = 1e-5$.\n",
    "7. Try $lambda$ is a range from around $1$ to $1e5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function \n",
    "    f = 1/(1+exp(-beta^T * x))\n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    return 1./(1 + np.exp(-x))\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps = 1e-9):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_pred are probabilities\n",
    "    \"\"\"\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1-y_true) * np.log(1-y_pred + eps))\n",
    "\n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(beta,2)\n",
    "\n",
    "def fair_loss(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Group fairness Loss\n",
    "    \"\"\"\n",
    "    n = y.shape[0]\n",
    "    n1 = np.sum(groups == 1)\n",
    "    n2 = np.sum(groups == 2)\n",
    "    \n",
    "    ## Solution with numpy\n",
    "    equal_pairs = np.argwhere(y[np.newaxis, :] == y[:, np.newaxis]) ### Check if labels are the same, distance is 1 if y[i] == y[j] and 0 if y[i] != y[j]\n",
    "    equal_pairs = equal_pairs[np.where(equal_pairs[:,0] != equal_pairs[:,1])] ### Remove the pairs where items are compared to themselves\n",
    "    diff_groups = (groups[equal_pairs[:,0]] != groups[equal_pairs[:,1]]).astype(int) ### Find pairs where groups are different\n",
    "    cost = diff_groups.dot((y_pred[equal_pairs[:,0]] - y_pred[equal_pairs[:,1]])**2)\n",
    "    cost /= (n1*n2)\n",
    "    return cost\n",
    "\n",
    "    # Alternatively, you can use for loops (slower)\n",
    "    cost = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j: ### Remove the pairs where items are compared to themselves\n",
    "                if y[i] == y[j]: ### Check if labels are the same, distance is 1 if y[i] == y[j] and 0 if y[i] != y[j]\n",
    "                    if groups[i] != groups[j]: ### Find pairs where groups are different\n",
    "                        cost += (y_pred[i] - y_pred[j])**2\n",
    "    cost /= (n1*n2)\n",
    "    return cost\n",
    "\n",
    "def compute_cost(beta, X, y, groups, _lambda, _gamma):\n",
    "    # Compute predicted probs\n",
    "    probs = sigmoid(X.dot(beta).astype(float))\n",
    "    # Compute joint loss\n",
    "    loss = logistic_loss(y, probs) + _lambda * fair_loss(y, X.dot(beta).astype(float), groups) + _gamma * l2_loss(beta[1:])\n",
    "    return loss\n",
    "\n",
    "def compute_gradients(beta, X, y, groups, _lambda, _gamma):\n",
    "    \"\"\"\n",
    "    Calculate the gradient - used for finding the best beta values\n",
    "    \"\"\"\n",
    "    # Start with empty gradient\n",
    "    grad = np.zeros(beta.shape)\n",
    "    \n",
    "    m = len(X) # Number of training samples\n",
    "    \n",
    "    # Calculate the probs\n",
    "    probs = sigmoid(X.dot(beta).astype(float))\n",
    "    \n",
    "    # Calculate gradients for each beta value\n",
    "    for j in range(len(grad)):\n",
    "        if j == 0: ### We do not want to regularize the intercept\n",
    "            grad[j] = (1/m) * (probs-y).dot(X[:,j])\n",
    "        else:\n",
    "            grad[j] = (1/m) * (probs-y).dot(X[:,j]) + (2*_gamma)*beta[j]\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.232930116363691"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cost with random beta-values and parameters\n",
    "compute_cost(\n",
    "    beta = np.random.rand(X_train_scaled.shape[1]),\n",
    "    X = X_train_scaled, \n",
    "    y = y_train,\n",
    "    groups = group_train, \n",
    "    _gamma = 1, \n",
    "    _lambda = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  NIT   NF   F                       GTG\n",
      "    0    1  1.132000253430093E+00   1.39544744E-01\n",
      "    1    7  7.134861485559513E-01   2.44059514E-02\n",
      "    2   15  6.037794338288999E-01   1.07520023E-02\n",
      "    3   46  6.015203010276727E-01   6.44979070E-03\n",
      "tnc: |fn-fn-1] = 0 -> convergence\n",
      "    4   95  6.015203010276727E-01   6.44979070E-03\n",
      "tnc: Converged (|f_n-f_(n-1)| ~= 0)\n"
     ]
    }
   ],
   "source": [
    "# Set seed and define params\n",
    "np.random.seed(0)\n",
    "beta = np.random.rand(X_train_scaled.shape[1])\n",
    "lambda_ = 1000\n",
    "gamma_ = 1e-5 \n",
    "\n",
    "# Run optimization\n",
    "result, _, _ = opt.fmin_tnc(\n",
    "    func=compute_cost,\n",
    "    x0=beta,\n",
    "    fprime=compute_gradients,\n",
    "    approx_grad=False,\n",
    "    maxfun = 500,\n",
    "    args = (\n",
    "        X_train_scaled, \n",
    "        y_train,\n",
    "        group_train,\n",
    "        lambda_, \n",
    "        gamma_\n",
    "    ),\n",
    "    xtol=1e-7,\n",
    "    ftol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
